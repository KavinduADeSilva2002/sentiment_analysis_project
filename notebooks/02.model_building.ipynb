{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c722114-5fd5-4cb3-a724-6da646c4f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f41fdc3-6828-4389-9dd9-c4626976fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../artifacts/sentiment_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db57dc8-e257-43c3-ba40-3dcfca7a58ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
       "1   2      0  Finally a transparant silicon case ^^ Thanks t...\n",
       "2   3      0  We love this! Would you go? #talk #makememorie...\n",
       "3   4      0  I'm wired I know I'm George I was made that wa...\n",
       "4   5      1  What amazing service! Apple won't even talk to..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2997284-0719-49f7-91b4-cb00cb1882d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d1db730-8daf-43f4-b4c6-1e8289e677db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.duplicated of         id  label                                              tweet\n",
       "0        1      0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
       "1        2      0  Finally a transparant silicon case ^^ Thanks t...\n",
       "2        3      0  We love this! Would you go? #talk #makememorie...\n",
       "3        4      0  I'm wired I know I'm George I was made that wa...\n",
       "4        5      1  What amazing service! Apple won't even talk to...\n",
       "...    ...    ...                                                ...\n",
       "7915  7916      0  Live out loud #lol #liveoutloud #selfie #smile...\n",
       "7916  7917      0  We would like to wish you an amazing day! Make...\n",
       "7917  7918      0  Helping my lovely 90 year old neighbor with he...\n",
       "7918  7919      0  Finally got my #smart #pocket #wifi stay conne...\n",
       "7919  7920      0  Apple Barcelona!!! #Apple #Store #BCN #Barcelo...\n",
       "\n",
       "[7920 rows x 3 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01effbb-6e05-437d-8e1f-14cb2f6622d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "7915    False\n",
       "7916    False\n",
       "7917    False\n",
       "7918    False\n",
       "7919    False\n",
       "Length: 7920, dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "731ac74a-8284-4bf8-86f6-d73ae8977627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c86a2e-432d-4a58-b35f-2f530891c585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       0\n",
       "label    0\n",
       "tweet    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ec97e-fd65-45ae-81ba-e21d5320643f",
   "metadata": {},
   "source": [
    "###Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adbcecc9-c2b9-42be-b48c-05eeb66845ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f502ae6-ab47-4f83-bde6-bf3617e29a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
       "1    Finally a transparant silicon case ^^ Thanks t...\n",
       "2    We love this! Would you go? #talk #makememorie...\n",
       "3    I'm wired I know I'm George I was made that wa...\n",
       "4    What amazing service! Apple won't even talk to...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f8560f-42ac-4d08-8f47-afa93fe219f7",
   "metadata": {},
   "source": [
    "convert uppercase to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "460b9e35-0ffa-456a-b7f4-ea7d8dacca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \"\".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cbbc706-5664-4119-8244-bf0c34278277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint#pregnancytesthttps://goo.gl/h1mfq...\n",
       "1    finallyatransparantsiliconcase^^thankstomyuncl...\n",
       "2    welovethis!wouldyougo?#talk#makememories#unplu...\n",
       "3    i'mwirediknowi'mgeorgeiwasmadethatway;)#iphone...\n",
       "4    whatamazingservice!applewon'teventalktomeabout...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee157b-e5d8-429c-b85b-ccf0e1dcff04",
   "metadata": {},
   "source": [
    "remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dedce9d1-03f1-4adf-b5f2-b9155eef958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \"\".join(re.sub(r'^https?:\\/\\/.*[\\r\\n] *','',x, flags=re.MULTILINE) for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e8e27ef-32de-4314-b783-5f1d05c67c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint#pregnancytesthttps://goo.gl/h1mfq...\n",
       "1    finallyatransparantsiliconcase^^thankstomyuncl...\n",
       "2    welovethis!wouldyougo?#talk#makememories#unplu...\n",
       "3    i'mwirediknowi'mgeorgeiwasmadethatway;)#iphone...\n",
       "4    whatamazingservice!applewon'teventalktomeabout...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f3a1457-53e9-402a-8d3e-0d96b191a104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint#pregnancytesthttps://goo.gl/h1mfq...\n",
       "1    finallyatransparantsiliconcase^^thankstomyuncl...\n",
       "2    welovethis!wouldyougo?#talk#makememories#unplu...\n",
       "3    i'mwirediknowi'mgeorgeiwasmadethatway;)#iphone...\n",
       "4    whatamazingservice!applewon'teventalktomeabout...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bab8ba9-88ef-479b-a057-de848203a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \"\".join(re.sub(r'^https?:\\/\\/.*[\\r\\n] *','',x, flags=re.MULTILINE) for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "326fdcfb-ad46-41e4-b72a-168b29d6975f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint#pregnancytesthttps://goo.gl/h1mfq...\n",
       "1    finallyatransparantsiliconcase^^thankstomyuncl...\n",
       "2    welovethis!wouldyougo?#talk#makememories#unplu...\n",
       "3    i'mwirediknowi'mgeorgeiwasmadethatway;)#iphone...\n",
       "4    whatamazingservice!applewon'teventalktomeabout...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30401c31-a12b-4f4f-84a4-bcf8b4f73656",
   "metadata": {},
   "source": [
    "Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "846345b0-99f6-449b-89c0-047f65280c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8236192b-9ff8-4d32-a3be-f8862412eff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2125279520.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef remove_punctuations(text) : for punctuation in string.punctuation: text = text.replace(punctuation, '') return text data[\"tweet\"] = data[\"tweet\"].apply(remove_punctuations)\u001b[39m\n                                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuations(text) : for punctuation in string.punctuation: text = text.replace(punctuation, '') return text data[\"tweet\"] = data[\"tweet\"].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98ff77aa-0a34-4f32-a10d-6da942fd7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text =text.replace(punctuation, '')\n",
    "        return text\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(remove_punctuations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e384707a-7007-4984-95ac-315c2c4fd067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint#pregnancytesthttps://goo.gl/h1mfq...\n",
       "1    finallyatransparantsiliconcase^^thankstomyuncl...\n",
       "2    welovethis!wouldyougo?#talk#makememories#unplu...\n",
       "3    i'mwirediknowi'mgeorgeiwasmadethatway;)#iphone...\n",
       "4    whatamazingservice!applewon'teventalktomeabout...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07a7eec8-11ad-401f-83e9-41d18945dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text =text.replace(punctuation, '')\n",
    "        return text\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a85bedc-2b90-409a-8da9-014ee5dbe1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint#pregnancytesthttps://goo.gl/h1mfq...\n",
       "1    finallyatransparantsiliconcase^^thankstomyuncl...\n",
       "2    welovethiswouldyougo?#talk#makememories#unplug...\n",
       "3    i'mwirediknowi'mgeorgeiwasmadethatway;)#iphone...\n",
       "4    whatamazingserviceapplewon'teventalktomeabouta...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce10cef6-dfca-49ce-bbd4-55c8c74c0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1b27be2-e340-4086-a0c4-b134a3151bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text =text.replace(punctuation, '')\n",
    "        return text\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fc22a01-42ae-4cc2-9846-a02786a8107b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint#pregnancytesthttps://goo.gl/h1mfq...\n",
       "1    finallyatransparantsiliconcase^^thankstomyuncl...\n",
       "2    welovethiswouldyougo?#talk#makememories#unplug...\n",
       "3    i'mwirediknowi'mgeorgeiwasmadethatway;)#iphone...\n",
       "4    whatamazingserviceapplewon'teventalktomeabouta...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5dea6873-69bb-43e3-861c-6304ddf88d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # remove punctuation/symbols\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, '')\n",
    "\n",
    "    # optional: remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f08c29b2-cacf-4a0f-90e3-ce3bd216da26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint#pregnancytesthttps://goo.gl/h1mfq...\n",
       "1    finallyatransparantsiliconcase^^thankstomyuncl...\n",
       "2    welovethiswouldyougo?#talk#makememories#unplug...\n",
       "3    i'mwirediknowi'mgeorgeiwasmadethatway;)#iphone...\n",
       "4    whatamazingserviceapplewon'teventalktomeabouta...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d9ddac7-c2ee-4b71-af47-1585d615af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "        return text   # ‚ùå WRONG place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "916af7de-99ac-42ae-b404-f0be1252b94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #fingerprint#pregnancytesthttps://goo.gl/h1mfq...\n",
       "1    finallyatransparantsiliconcase^^thankstomyuncl...\n",
       "2    welovethiswouldyougo?#talk#makememories#unplug...\n",
       "3    i'mwirediknowi'mgeorgeiwasmadethatway;)#iphone...\n",
       "4    whatamazingserviceapplewon'teventalktomeabouta...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60d627f1-9efc-4dcf-a6d5-cef01cbb84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(remove_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8755420-6bbe-4677-a5a1-3554a0f94912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           #fingerprint#pregnancytest\n",
       "1    finallyatransparantsiliconcase^^thankstomyuncl...\n",
       "2    welovethiswouldyougo?#talk#makememories#unplug...\n",
       "3    i'mwirediknowi'mgeorgeiwasmadethatway;)#iphone...\n",
       "4    whatamazingserviceapplewon'teventalktomeabouta...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adecd371-dc39-4e77-ad47-1447cb73e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_tweet(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)   # remove links\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove symbols\n",
    "    return text\n",
    "\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4fcab5e-48aa-44c8-8623-add99a6ac039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             fingerprintpregnancytest\n",
       "1    finallyatransparantsiliconcasethankstomyuncley...\n",
       "2    welovethiswouldyougotalkmakememoriesunplugrela...\n",
       "3    imwirediknowimgeorgeiwasmadethatwayiphonecuted...\n",
       "4    whatamazingserviceapplewonteventalktomeaboutaq...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52de7e34-6f82-4020-b48b-161edd7ee914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7910    perfectmatchinstagoodapplewatchredinstagramiph...\n",
       "7911    iamcompletelyinlovewiththenewiphoneemojisiphon...\n",
       "7912                       tuneinturnondropoutgtdinoneapp\n",
       "7913    oksomygalaxycrashedafteronedaynowihavetowaitti...\n",
       "7914    gainfollowersrtthismustfollowmeifollowbackfoll...\n",
       "7915    liveoutloudlolliveoutloudselfiesmilesonymusich...\n",
       "7916    wewouldliketowishyouanamazingdaymakeeveryminut...\n",
       "7917    helpingmylovely90yearoldneighborwithheripadthi...\n",
       "7918    finallygotmysmartpocketwifistayconnectedanytim...\n",
       "7919    applebarcelonaapplestorebcnbarcelonatravelipho...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f83e107b-3711-4832-85bf-0ff6f991d96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "<>:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19696\\245014465.py:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "  data[\"tweet\"] =data[\"tweet\"].str.replace('\\d+','',regex=True)\n"
     ]
    }
   ],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].str.replace('\\d+','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42e34ea1-41f1-4060-a312-74864c25fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "<>:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19696\\1058580944.py:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "  data[\"tweet\"] = data[\"tweet\"].str.replace('\\d+','',regex=True)\n"
     ]
    }
   ],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].str.replace('\\d+','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5359712a-2c1c-48fc-94cf-6818bcd16a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "<>:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19696\\1058580944.py:1: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "  data[\"tweet\"] = data[\"tweet\"].str.replace('\\d+','',regex=True)\n"
     ]
    }
   ],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].str.replace('\\d+','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bcef9506-76db-43c9-b9e7-0d3ef9c6ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].str.replace(r'\\d+', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1a3cdf6-af0e-4363-b092-0d2970758feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7910    perfectmatchinstagoodapplewatchredinstagramiph...\n",
       "7911    iamcompletelyinlovewiththenewiphoneemojisiphon...\n",
       "7912                       tuneinturnondropoutgtdinoneapp\n",
       "7913    oksomygalaxycrashedafteronedaynowihavetowaitti...\n",
       "7914    gainfollowersrtthismustfollowmeifollowbackfoll...\n",
       "7915    liveoutloudlolliveoutloudselfiesmilesonymusich...\n",
       "7916    wewouldliketowishyouanamazingdaymakeeveryminut...\n",
       "7917    helpingmylovelyyearoldneighborwithheripadthism...\n",
       "7918    finallygotmysmartpocketwifistayconnectedanytim...\n",
       "7919    applebarcelonaapplestorebcnbarcelonatravelipho...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8f4ac-aee5-4835-8dab-91f4b533efc2",
   "metadata": {},
   "source": [
    "remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29103e6d-3744-4aa0-8ec7-c6b3c2a6bb1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "090829bb-ca72-421c-a415-ecc93cbbede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp314-cp314-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in e:\\github\\sentiment_analysis_project\\env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.3 MB/s  0:00:00\n",
      "Downloading regex-2025.11.3-cp314-cp314-win_amd64.whl (280 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   ---------------------------------------- 5/5 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.1 joblib-1.5.3 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f2c40fb-25c2-4f34-a848-d5e359f46df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "904632c2-1939-4296-a1a4-861b69cf91eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ../static/modal...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords',download_dir='../static/modal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88d52908-b5c2-40b4-9a30-e3051c0f38f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../static/model.corpora/stopwords/english'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../static/model.corpora/stopwords/english\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      2\u001b[39m     sw = file.read().splitlines()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../static/model.corpora/stopwords/english'"
     ]
    }
   ],
   "source": [
    "with open('../static/model.corpora/stopwords/english', 'r') as file:\n",
    "    sw = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4a5d72f-34df-499a-ac46-a0246b9df103",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../static/modal.corpora/stopwords/english'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../static/modal.corpora/stopwords/english\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      2\u001b[39m     sw = file.read().splitlines()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../static/modal.corpora/stopwords/english'"
     ]
    }
   ],
   "source": [
    "with open('../static/modal.corpora/stopwords/english', 'r') as file:\n",
    "    sw = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "279e8344-60cb-451b-9e17-11c6de7dbe74",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../static/model.corpora/stopwords/english'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../static/model.corpora/stopwords/english\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      2\u001b[39m     sw = file.read().splitlines()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../static/model.corpora/stopwords/english'"
     ]
    }
   ],
   "source": [
    "with open('../static/model.corpora/stopwords/english', 'r') as file:\n",
    "    sw = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "161e210c-ad75-4a2b-b33b-a8a23508befe",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Lenovo/nltk_data'\n    - 'E:\\\\Github\\\\sentiment_analysis_project\\\\env\\\\nltk_data'\n    - 'E:\\\\Github\\\\sentiment_analysis_project\\\\env\\\\share\\\\nltk_data'\n    - 'E:\\\\Github\\\\sentiment_analysis_project\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Lenovo/nltk_data'\n    - 'E:\\\\Github\\\\sentiment_analysis_project\\\\env\\\\nltk_data'\n    - 'E:\\\\Github\\\\sentiment_analysis_project\\\\env\\\\share\\\\nltk_data'\n    - 'E:\\\\Github\\\\sentiment_analysis_project\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sw = \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Lenovo/nltk_data'\n    - 'E:\\\\Github\\\\sentiment_analysis_project\\\\env\\\\nltk_data'\n    - 'E:\\\\Github\\\\sentiment_analysis_project\\\\env\\\\share\\\\nltk_data'\n    - 'E:\\\\Github\\\\sentiment_analysis_project\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d58ea55a-440c-4e9d-9417-ad6961e678b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41e6bccc-97fa-4db8-bcfe-9cee54683952",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../static/modal.corpora/stopwords/english'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../static/modal.corpora/stopwords/english\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      2\u001b[39m     sw = file.read().splitlines()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../static/modal.corpora/stopwords/english'"
     ]
    }
   ],
   "source": [
    "with open('../static/modal.corpora/stopwords/english', 'r') as file:\n",
    "    sw = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6ce3056-961b-4862-9764-a6f5044900ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../static/model.corpora/stopwords/english'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../static/model.corpora/stopwords/english\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      2\u001b[39m     sw = file.read().splitlines()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../static/model.corpora/stopwords/english'"
     ]
    }
   ],
   "source": [
    "with open('../static/model.corpora/stopwords/english', 'r') as file:\n",
    "    sw = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8062cbd2-d875-44f3-8634-e989afd673ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "824a7055-3f7b-4a6e-b583-fce44f16aacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "stopwords_file_path = os.path.join(stopwords.root.path, 'english')\n",
    "\n",
    "with open(stopwords_file_path, 'r', encoding='utf-8') as file:\n",
    "    sw = file.read().splitlines()\n",
    "\n",
    "print(sw[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddddb04d-055d-4ad4-891a-6b39992c7cb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(filtered_words)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Apply to the column\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].apply(remove_stopwords)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Now df['clean_text'] has text without stopwords\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m'\u001b[39m]].head())\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to clean a single text\n",
    "def remove_stopwords(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    words = word_tokenize(str(text).lower())\n",
    "    filtered_words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply to the column\n",
    "df['clean_text'] = df['text'].apply(remove_stopwords)\n",
    "\n",
    "# Now df['clean_text'] has text without stopwords\n",
    "print(df[['text', 'clean_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "98b73718-0e50-42ab-95f4-e876e1afa371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to clean a single text\n",
    "def remove_stopwords(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    words = word_tokenize(str(text).lower())\n",
    "    filtered_words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9e7e4a1-c7c0-4409-b709-bf08226e7f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7910    perfectmatchinstagoodapplewatchredinstagramiph...\n",
       "7911    iamcompletelyinlovewiththenewiphoneemojisiphon...\n",
       "7912                       tuneinturnondropoutgtdinoneapp\n",
       "7913    oksomygalaxycrashedafteronedaynowihavetowaitti...\n",
       "7914    gainfollowersrtthismustfollowmeifollowbackfoll...\n",
       "7915    liveoutloudlolliveoutloudselfiesmilesonymusich...\n",
       "7916    wewouldliketowishyouanamazingdaymakeeveryminut...\n",
       "7917    helpingmylovelyyearoldneighborwithheripadthism...\n",
       "7918    finallygotmysmartpocketwifistayconnectedanytim...\n",
       "7919    applebarcelonaapplestorebcnbarcelonatravelipho...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf134a45-07fb-4cae-aeda-9512e4be5c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             fingerprintpregnancytest\n",
       "1    finallyatransparantsiliconcasethankstomyuncley...\n",
       "2    welovethiswouldyougotalkmakememoriesunplugrela...\n",
       "3    imwirediknowimgeorgeiwasmadethatwayiphonecuted...\n",
       "4    whatamazingserviceapplewonteventalktomeaboutaq...\n",
       "5    iphonesoftwareupdatefuckedupmyphonebigtimestup...\n",
       "6      happyforusinstapicinstadailyussonyxperiaxperiaz\n",
       "7                               newtypecchargercableuk\n",
       "8    bouttogoshoppingagainlisteningtomusiciphonejus...\n",
       "9    photofunselfiepoolwatersonycamerapicofthedaysu...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a881b-e547-4b20-bf55-805f6ca3fd49",
   "metadata": {},
   "source": [
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc6f3075-97b1-4b8e-bf1d-a29f0bb03ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ab25892-e70d-4917-b955-6abacf98f1b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PorterStemmer.stem() missing 1 required positional argument: 'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33mtweet\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtweet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mps\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\pandas\\core\\series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Github\\sentiment_analysis_project\\env\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33mtweet\u001b[39m\u001b[33m\"\u001b[39m] = data[\u001b[33m\"\u001b[39m\u001b[33mtweet\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mps\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33mtweet\u001b[39m\u001b[33m\"\u001b[39m] = data[\u001b[33m\"\u001b[39m\u001b[33mtweet\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[43mps\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x.split()))\n",
      "\u001b[31mTypeError\u001b[39m: PorterStemmer.stem() missing 1 required positional argument: 'word'"
     ]
    }
   ],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x:\" \".join(ps.stem(x) for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "724b393e-40f1-4121-9f90-f7fbfabc9747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7910    perfectmatchinstagoodapplewatchredinstagramiph...\n",
       "7911    iamcompletelyinlovewiththenewiphoneemojisiphon...\n",
       "7912                       tuneinturnondropoutgtdinoneapp\n",
       "7913    oksomygalaxycrashedafteronedaynowihavetowaitti...\n",
       "7914    gainfollowersrtthismustfollowmeifollowbackfoll...\n",
       "7915    liveoutloudlolliveoutloudselfiesmilesonymusich...\n",
       "7916    wewouldliketowishyouanamazingdaymakeeveryminut...\n",
       "7917    helpingmylovelyyearoldneighborwithheripadthism...\n",
       "7918    finallygotmysmartpocketwifistayconnectedanytim...\n",
       "7919    applebarcelonaapplestorebcnbarcelonatravelipho...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1781816-d8f6-430e-ad5c-d540919428c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()  # ‚Üê Important: instantiate with ()\n",
    "\n",
    "# Define a function to stem a single tweet (handles NaN and non-string safely)\n",
    "def stem_tweet(text):\n",
    "    if isinstance(text, str):  # Only process if it's actually a string\n",
    "        return \" \".join(ps.stem(word) for word in text.split())\n",
    "    else:\n",
    "        return text  # Return NaN or original value unchanged\n",
    "\n",
    "# Apply it\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(stem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00f31836-48d0-40b5-952e-5b715f1c8a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7910    perfectmatchinstagoodapplewatchredinstagramiph...\n",
       "7911    iamcompletelyinlovewiththenewiphoneemojisiphon...\n",
       "7912                       tuneinturnondropoutgtdinoneapp\n",
       "7913    oksomygalaxycrashedafteronedaynowihavetowaitti...\n",
       "7914    gainfollowersrtthismustfollowmeifollowbackfoll...\n",
       "7915    liveoutloudlolliveoutloudselfiesmilesonymusich...\n",
       "7916    wewouldliketowishyouanamazingdaymakeeveryminut...\n",
       "7917    helpingmylovelyyearoldneighborwithheripadthism...\n",
       "7918    finallygotmysmartpocketwifistayconnectedanytim...\n",
       "7919    applebarcelonaapplestorebcnbarcelonatravelipho...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed316b9a-1724-4dfd-9a14-4bd56225c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(ps.stem(word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5fc3b9d7-ab80-4f15-a409-cad344d6f916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7910    perfectmatchinstagoodapplewatchredinstagramiph...\n",
       "7911    iamcompletelyinlovewiththenewiphoneemojisiphon...\n",
       "7912                       tuneinturnondropoutgtdinoneapp\n",
       "7913    oksomygalaxycrashedafteronedaynowihavetowaitti...\n",
       "7914    gainfollowersrtthismustfollowmeifollowbackfoll...\n",
       "7915    liveoutloudlolliveoutloudselfiesmilesonymusich...\n",
       "7916    wewouldliketowishyouanamazingdaymakeeveryminut...\n",
       "7917    helpingmylovelyyearoldneighborwithheripadthism...\n",
       "7918    finallygotmysmartpocketwifistayconnectedanytim...\n",
       "7919    applebarcelonaapplestorebcnbarcelonatravelipho...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d2da9-a546-44bc-8a06-027d8d7ed1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
